{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>–¢–∏–ø –æ—Ç–∑—ã–≤–∞</th>\n",
       "      <th>–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞</th>\n",
       "      <th>–°–ø–∞–º</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π</td>\n",
       "      <td>üëçüëçüëç–°—É–ø–µ—Ä —Ä–∞–∑—ä*–± üëçüëçüëçüëçüëç</td>\n",
       "      <td>–°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π</td>\n",
       "      <td>–õ–∞–∑–∏–ª–∞ –ª–∞–ø–æ–≤–æ–µ –∫–∞—à–∞ –∞ –ø—Ä–∏–ª–∏–ø –ø–∏–∞–ª</td>\n",
       "      <td>–°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π</td>\n",
       "      <td>–ù—É —á—Ç–æ –∫–∞–∫ —É –Ω–∞—Å —Å –≤–∞–º–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å —ç—Ç–∏–º —Ä–∞–±–æ...</td>\n",
       "      <td>–°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π</td>\n",
       "      <td>–ù—É —á—Ç–æ —Ç–∞–º –∫–∞–∫ —Ç–∞–º –¥–µ–ª–∞ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —É</td>\n",
       "      <td>–°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π</td>\n",
       "      <td>–ù—É —á—Ç–æ —Ç–∞–º –∫–∞–∫ —Ç–∞–º –¥–µ–ª–∞ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —É –≤–∞—Å —á...</td>\n",
       "      <td>–°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π</td>\n",
       "      <td>–£–∂–∞—Å–Ω–æ.\\n–ñ–¥–∞–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ \\n–ù–∏—á–µ–≥–æ —Ç–∞–∫ –∏ ...</td>\n",
       "      <td>–ù–µ –¥–æ—Å—Ç–∞–≤–ª–µ–Ω</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π</td>\n",
       "      <td>–£–∂–∞—Å–Ω—ã–π —Å–µ—Ä–≤–∏—Å !!!\\n–î–æ—Å—Ç–∞–≤–∫–∞ –µ—Ö–∞–ª–∞ –ø–æ—á—Ç–∏ —Ç—Ä–∏ —á...</td>\n",
       "      <td>–ù–µ –¥–æ–≤–µ–∑–ª–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π</td>\n",
       "      <td>–ê–∫—Ü–∏—è –ø–∏—Ü—Ü–∞ –≤ –ø–æ–¥–∞—Ä–æ–∫ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç</td>\n",
       "      <td>–î—Ä—É–≥–æ–µ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2212</th>\n",
       "      <td>–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π</td>\n",
       "      <td>–Ω–µ —É–≤–∏–¥–µ–ª –∏–º–±–∏—Ä—èüòî\\n–Ω–µ —É–≤–∏–¥–µ–ª –≤–∞—Å–∞–±–∏üò©</td>\n",
       "      <td>–£—Ç–æ—á–Ω–µ–Ω–∏—è –ø–æ –æ—Ç–∑—ã–≤–∞–º (–¥–æ–ø—ã)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π</td>\n",
       "      <td>–Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –æ–¥–Ω–æ–≥–æ –¶–µ–∑–∞—Ä—å —Ç–µ–º–ø—É—Ä–∞</td>\n",
       "      <td>–ù–µ –¥–æ–≤–µ–∑–ª–∏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2214 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      –¢–∏–ø –æ—Ç–∑—ã–≤–∞                                       –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞  \\\n",
       "0     –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π                              üëçüëçüëç–°—É–ø–µ—Ä —Ä–∞–∑—ä*–± üëçüëçüëçüëçüëç   \n",
       "1     –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π                  –õ–∞–∑–∏–ª–∞ –ª–∞–ø–æ–≤–æ–µ –∫–∞—à–∞ –∞ –ø—Ä–∏–ª–∏–ø –ø–∏–∞–ª   \n",
       "2     –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π  –ù—É —á—Ç–æ –∫–∞–∫ —É –Ω–∞—Å —Å –≤–∞–º–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å —ç—Ç–∏–º —Ä–∞–±–æ...   \n",
       "3     –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π           –ù—É —á—Ç–æ —Ç–∞–º –∫–∞–∫ —Ç–∞–º –¥–µ–ª–∞ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —É   \n",
       "4     –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π  –ù—É —á—Ç–æ —Ç–∞–º –∫–∞–∫ —Ç–∞–º –¥–µ–ª–∞ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —É –≤–∞—Å —á...   \n",
       "...          ...                                                ...   \n",
       "2209  –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π  –£–∂–∞—Å–Ω–æ.\\n–ñ–¥–∞–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ \\n–ù–∏—á–µ–≥–æ —Ç–∞–∫ –∏ ...   \n",
       "2210  –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π  –£–∂–∞—Å–Ω—ã–π —Å–µ—Ä–≤–∏—Å !!!\\n–î–æ—Å—Ç–∞–≤–∫–∞ –µ—Ö–∞–ª–∞ –ø–æ—á—Ç–∏ —Ç—Ä–∏ —á...   \n",
       "2211  –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π                  –ê–∫—Ü–∏—è –ø–∏—Ü—Ü–∞ –≤ –ø–æ–¥–∞—Ä–æ–∫ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç   \n",
       "2212  –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π               –Ω–µ —É–≤–∏–¥–µ–ª –∏–º–±–∏—Ä—èüòî\\n–Ω–µ —É–≤–∏–¥–µ–ª –≤–∞—Å–∞–±–∏üò©   \n",
       "2213  –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π                   –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –æ–¥–Ω–æ–≥–æ –¶–µ–∑–∞—Ä—å —Ç–µ–º–ø—É—Ä–∞   \n",
       "\n",
       "                             –°–ø–∞–º  \n",
       "0                   –°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å  \n",
       "1                   –°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å  \n",
       "2                   –°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å  \n",
       "3                   –°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å  \n",
       "4                   –°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å  \n",
       "...                           ...  \n",
       "2209                 –ù–µ –¥–æ—Å—Ç–∞–≤–ª–µ–Ω  \n",
       "2210                   –ù–µ –¥–æ–≤–µ–∑–ª–∏  \n",
       "2211                       –î—Ä—É–≥–æ–µ  \n",
       "2212  –£—Ç–æ—á–Ω–µ–Ω–∏—è –ø–æ –æ—Ç–∑—ã–≤–∞–º (–¥–æ–ø—ã)  \n",
       "2213                   –ù–µ –¥–æ–≤–µ–∑–ª–∏  \n",
       "\n",
       "[2214 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('–û—Ç–∑—ã–≤—ã_—Å–ø–∞–º,_–º–æ–¥–µ—Ä–∞—Ü–∏—è,_–ø—Ä–æ–±–ª–µ–º—ã,_–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ.xlsx')\n",
    "data.rename(columns={'Unnamed: 0': '–¢–∏–ø –æ—Ç–∑—ã–≤–∞','Unnamed: 1' : '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞', 'Unnamed: 2': '–°–ø–∞–º'}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>–¢–∏–ø –æ—Ç–∑—ã–≤–∞</th>\n",
       "      <th>–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞</th>\n",
       "      <th>–°–ø–∞–º</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>üëçüëçüëç–°—É–ø–µ—Ä —Ä–∞–∑—ä*–± üëçüëçüëçüëçüëç</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>–õ–∞–∑–∏–ª–∞ –ª–∞–ø–æ–≤–æ–µ –∫–∞—à–∞ –∞ –ø—Ä–∏–ª–∏–ø –ø–∏–∞–ª</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>–ù—É —á—Ç–æ –∫–∞–∫ —É –Ω–∞—Å —Å –≤–∞–º–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å —ç—Ç–∏–º —Ä–∞–±–æ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>–ù—É —á—Ç–æ —Ç–∞–º –∫–∞–∫ —Ç–∞–º –¥–µ–ª–∞ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —É</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>–ù—É —á—Ç–æ —Ç–∞–º –∫–∞–∫ —Ç–∞–º –¥–µ–ª–∞ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —É –≤–∞—Å —á...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>0</td>\n",
       "      <td>–£–∂–∞—Å–Ω–æ.\\n–ñ–¥–∞–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ \\n–ù–∏—á–µ–≥–æ —Ç–∞–∫ –∏ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>0</td>\n",
       "      <td>–£–∂–∞—Å–Ω—ã–π —Å–µ—Ä–≤–∏—Å !!!\\n–î–æ—Å—Ç–∞–≤–∫–∞ –µ—Ö–∞–ª–∞ –ø–æ—á—Ç–∏ —Ç—Ä–∏ —á...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>1</td>\n",
       "      <td>–ê–∫—Ü–∏—è –ø–∏—Ü—Ü–∞ –≤ –ø–æ–¥–∞—Ä–æ–∫ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2212</th>\n",
       "      <td>1</td>\n",
       "      <td>–Ω–µ —É–≤–∏–¥–µ–ª –∏–º–±–∏—Ä—èüòî\\n–Ω–µ —É–≤–∏–¥–µ–ª –≤–∞—Å–∞–±–∏üò©</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>1</td>\n",
       "      <td>–Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –æ–¥–Ω–æ–≥–æ –¶–µ–∑–∞—Ä—å —Ç–µ–º–ø—É—Ä–∞</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2214 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      –¢–∏–ø –æ—Ç–∑—ã–≤–∞                                       –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞  –°–ø–∞–º\n",
       "0              1                              üëçüëçüëç–°—É–ø–µ—Ä —Ä–∞–∑—ä*–± üëçüëçüëçüëçüëç     1\n",
       "1              1                  –õ–∞–∑–∏–ª–∞ –ª–∞–ø–æ–≤–æ–µ –∫–∞—à–∞ –∞ –ø—Ä–∏–ª–∏–ø –ø–∏–∞–ª     1\n",
       "2              1  –ù—É —á—Ç–æ –∫–∞–∫ —É –Ω–∞—Å —Å –≤–∞–º–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å —ç—Ç–∏–º —Ä–∞–±–æ...     1\n",
       "3              1           –ù—É —á—Ç–æ —Ç–∞–º –∫–∞–∫ —Ç–∞–º –¥–µ–ª–∞ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —É     1\n",
       "4              1  –ù—É —á—Ç–æ —Ç–∞–º –∫–∞–∫ —Ç–∞–º –¥–µ–ª–∞ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —É –≤–∞—Å —á...     1\n",
       "...          ...                                                ...   ...\n",
       "2209           0  –£–∂–∞—Å–Ω–æ.\\n–ñ–¥–∞–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ \\n–ù–∏—á–µ–≥–æ —Ç–∞–∫ –∏ ...     0\n",
       "2210           0  –£–∂–∞—Å–Ω—ã–π —Å–µ—Ä–≤–∏—Å !!!\\n–î–æ—Å—Ç–∞–≤–∫–∞ –µ—Ö–∞–ª–∞ –ø–æ—á—Ç–∏ —Ç—Ä–∏ —á...     0\n",
       "2211           1                  –ê–∫—Ü–∏—è –ø–∏—Ü—Ü–∞ –≤ –ø–æ–¥–∞—Ä–æ–∫ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç     0\n",
       "2212           1               –Ω–µ —É–≤–∏–¥–µ–ª –∏–º–±–∏—Ä—èüòî\\n–Ω–µ —É–≤–∏–¥–µ–ª –≤–∞—Å–∞–±–∏üò©     0\n",
       "2213           1                   –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –æ–¥–Ω–æ–≥–æ –¶–µ–∑–∞—Ä—å —Ç–µ–º–ø—É—Ä–∞     0\n",
       "\n",
       "[2214 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['–°–ø–∞–º'] = data['–°–ø–∞–º'].apply(lambda x: 1 if x == '–°–ø–∞–º, —É–¥–∞–ª–∏—Ç—å' else 0)\n",
    "data['–¢–∏–ø –æ—Ç–∑—ã–≤–∞'] = data['–¢–∏–ø –æ—Ç–∑—ã–≤–∞'].apply(lambda x: 1 if x == '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π' else 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190cfda471e346979d349672e22ad8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1771 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f546fdf034a4bcc9fc10e1edc3d1b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_423266/2958757075.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='333' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/333 01:32 < 1:11:23, 0.08 it/s, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from datasets import Dataset as HFDataset, DatasetDict  # Hugging Face Datasets\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ RuBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"DeepPavlov/rubert-base-cased\", num_labels=len(data['–°–ø–∞–º'].unique()))\n",
    "\n",
    "# –õ–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "    words = text.split()\n",
    "    lemmatized_text = \" \".join(morph.parse(word)[0].normal_form for word in words)\n",
    "    return lemmatized_text\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"text\": self.texts[idx], \"label\": self.labels[idx]}\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "X = data['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞'].apply(preprocess_text).tolist()\n",
    "y = data['–°–ø–∞–º'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ Hugging Face\n",
    "train_dataset = HFDataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "test_dataset = HFDataset.from_dict({\"text\": X_test, \"label\": y_test})\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": train_dataset.map(tokenize_function, batched=True),\n",
    "    \"test\": test_dataset.map(tokenize_function, batched=True),\n",
    "})\n",
    "\n",
    "# –£–¥–∞–ª–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–æ–Ω–∏ –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω—ã –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    evaluation_strategy=\"epoch\",    # –û—Ü–µ–Ω–∏–≤–∞—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',           # –õ–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "trainer.train()\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student-math-01/anaconda3/envs/py33/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",          # –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "#     evaluation_strategy=\"epoch\",    # –û—Ü–µ–Ω–∏–≤–∞—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     logging_dir='./logs',           # –õ–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "#     logging_steps=10,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_383190/817050193.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='333' max='333' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [333/333 19:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.183300</td>\n",
       "      <td>0.145144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.126015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>0.169506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=333, training_loss=0.07242698527246087, metrics={'train_runtime': 1165.0184, 'train_samples_per_second': 4.56, 'train_steps_per_second': 0.286, 'total_flos': 349477259281920.0, 'train_loss': 0.07242698527246087, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16950581967830658, 'eval_runtime': 24.6686, 'eval_samples_per_second': 17.958, 'eval_steps_per_second': 1.135, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# metrics = trainer.evaluate()\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9661399548532731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      0.98      0.98       389\n",
      "     Class 1       0.87      0.85      0.86        54\n",
      "\n",
      "    accuracy                           0.97       443\n",
      "   macro avg       0.92      0.92      0.92       443\n",
      "weighted avg       0.97      0.97      0.97       443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "# predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "# pred_labels = predictions.predictions.argmax(axis=1)\n",
    "# true_labels = predictions.label_ids\n",
    "\n",
    "# # –í—ã–≤–æ–¥ —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "# accuracy = accuracy_score(true_labels, pred_labels)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "# report = classification_report(true_labels, pred_labels, target_names=[\"Class 0\", \"Class 1\"])\n",
    "# print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# –£–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "save_path = \"model1.pth\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"config\": model.config,\n",
    "}, save_path)\n",
    "print(f\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['–¢–∏–ø –æ—Ç–∑—ã–≤–∞'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ Hugging Face\n",
    "train_dataset = HFDataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "test_dataset = HFDataset.from_dict({\"text\": X_test, \"label\": y_test})\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": train_dataset.map(tokenize_function, batched=True),\n",
    "    \"test\": test_dataset.map(tokenize_function, batched=True),\n",
    "})\n",
    "\n",
    "# –£–¥–∞–ª–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–æ–Ω–∏ –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω—ã –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    evaluation_strategy=\"epoch\",    # –û—Ü–µ–Ω–∏–≤–∞—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',           # –õ–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "trainer.train()\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# –£–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "save_path = \"model2.pth\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"config\": model.config,\n",
    "}, save_path)\n",
    "print(f\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py33",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
